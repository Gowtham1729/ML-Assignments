{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1a. For the following dataset shown in Figure 1( same as the one we used in the class), now let us assume that date is also an input feature. Which attribute will be chosen as the root node for the decision tree? Do you think choosing this attribute is a good choice? Explain your answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlook will be chosen as the attribute of the root node even in this case due to high gain of the outlook.Since, there is no correlation between the day and playing tennis, it is not a good choice to select the day as one of the feature. In this case day is just a random value with no relation to playing tennis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1b. Handling missing values in decision trees: Assume that the Outlook of D3 is missing. Could you still learn\n",
    "a decision tree from this example. The set of features of interest are: Outlook, Temperature, Humidity and\n",
    "Wind. Clarifiying again - Day is not to be used as a feature in this sub part of the question. Refer to Tom\n",
    "Mitchell Chapter 3.7.4 last paragraph and show the learnt decision tree despite the missing Outlook values\n",
    "from D3. [1 mark] **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the outlook of D3 is missing, we can either remove that entry or replace some other value. Missing values are generally replaced by average of all other values of that feature. However the data for outlook cannot be averaged because they are discrete non numerical feature.\n",
    "    If we remove D3 coloumn, we will get a new dataset without D3 coloumn. For this new dataset, the information gain for the *Outlook* attribute is still higher and we will end up choosing the same root node and rest of the tree grows in the similar to the old tree. Hence we will end up with same decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2a. Write a Jupyter notebook to create a decision tree from scratch using the CART algorithm or using the algorithm taught in the class. The code should be written in native Python and not use existing libraries. The code should work for regression and classification tasks. As a hint: you may want to use dictionaries to encode the nested relationship amongst the different nodes, eg. tree = feature1 :{'val1': ..., 'val2', ...}}. [4 marks] **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
